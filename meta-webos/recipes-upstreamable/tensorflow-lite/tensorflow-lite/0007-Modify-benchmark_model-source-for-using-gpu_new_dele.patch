From ea690adc2c5e523176db148c3e62c9ad337f1d33 Mon Sep 17 00:00:00 2001
From: Jooyeon Lee <jylee256.lee@lge.com>
Date: Wed, 13 Jul 2022 20:38:55 +0900
Subject: [PATCH] Modify benchmark_model source for using gpu_new_delegates

:Release Notes:
Modify benchmark_model source for using gpu_new_delegates

:Detailed Notes:
Modify benchmark_model source for using gpu_new_delegates
<Added Options>
--use_gpu_new=true
--precision_loss_allowed=true
--inference_priority=MAX_PRECISION
--binary_cache_path="ABCD.bin"

:Testing Performed:
Local Benchmark Test

:QA Notes:
N/A

:Issues Addressed:
N/A

Change-Id: I0143ff8f3364cf079d1f119af585df152e24b876
Reviewed-on: http://gpro.lge.com/c/webos-pro/tensorflow-webos/+/333073
Reviewed-by: Commit Msg Checker <commit_msg@lge.com>
Reviewed-by: Ban Word Checker <ban_word@lge.com>
Reviewed-by: <jylee256.lee@lge.com>
Reviewed-by: <kijoong.lee@lge.com>
Tested-by: <jylee256.lee@lge.com>
Tested-by: <kijoong.lee@lge.com>
---
 tensorflow/lite/CMakeLists.txt                |   5 +-
 .../lite/tools/benchmark/CMakeLists.txt       |   2 +
 .../tools/benchmark/benchmark_tflite_model.cc |   6 +
 tensorflow/lite/tools/delegates/BUILD         |  12 ++
 .../lite/tools/delegates/delegate_provider.h  |   2 +
 .../delegates/gpu_new_delegate_provider.cc    | 186 ++++++++++++++++++
 tensorflow/lite/tools/evaluation/BUILD        |   1 +
 tensorflow/lite/tools/evaluation/utils.cc     |   8 +
 tensorflow/lite/tools/evaluation/utils.h      |   2 +
 9 files changed, 222 insertions(+), 2 deletions(-)
 create mode 100644 tensorflow/lite/tools/delegates/gpu_new_delegate_provider.cc

diff --git a/tensorflow/lite/CMakeLists.txt b/tensorflow/lite/CMakeLists.txt
index ada4f099d03..ce40c65627d 100644
--- a/tensorflow/lite/CMakeLists.txt
+++ b/tensorflow/lite/CMakeLists.txt
@@ -223,7 +223,8 @@ if(TFLITE_ENABLE_GPU)
   endif()
   populate_tflite_source_vars(
     "delegates/gpu/cl" TFLITE_DELEGATES_GPU_CL_SRCS
-    FILTER "(_test|gl_interop|gpu_api_delegate|egl_sync)\\.(cc|h)$"
+    #FILTER "(_test|gl_interop|gpu_api_delegate|egl_sync)\\.(cc|h)$"
+    FILTER "(_test|gl_interop|egl_sync)\\.(cc|h)$"
   )
   populate_tflite_source_vars(
     "delegates/gpu/cl/default" TFLITE_DELEGATES_GPU_CL_DEFAULT_SRCS
@@ -463,7 +464,7 @@ endif()
 add_subdirectory(${TFLITE_SOURCE_DIR}/tools/benchmark)
 
 # The label_image example.
-add_subdirectory(${TFLITE_SOURCE_DIR}/examples/label_image)
+#add_subdirectory(${TFLITE_SOURCE_DIR}/examples/label_image)
 
 # Python interpreter wrapper.
 add_library(_pywrap_tensorflow_interpreter_wrapper SHARED EXCLUDE_FROM_ALL
diff --git a/tensorflow/lite/tools/benchmark/CMakeLists.txt b/tensorflow/lite/tools/benchmark/CMakeLists.txt
index 11a92b4e0a8..43d1af80dfc 100644
--- a/tensorflow/lite/tools/benchmark/CMakeLists.txt
+++ b/tensorflow/lite/tools/benchmark/CMakeLists.txt
@@ -35,6 +35,7 @@ list(APPEND TFLITE_BENCHMARK_SRCS
 )
 
 list(APPEND TFLITE_BENCHMARK_LIBS
+  -lstdc++fs
   tensorflow-lite
 )
 
@@ -68,6 +69,7 @@ endif()  # Android
 if(TFLITE_ENABLE_GPU)
   list(APPEND TFLITE_BENCHMARK_SRCS
     ${TFLITE_SOURCE_DIR}/tools/delegates/gpu_delegate_provider.cc
+    ${TFLITE_SOURCE_DIR}/tools/delegates/gpu_new_delegate_provider.cc
   )
 endif()  # TFLITE_ENABLE_GPU
 
diff --git a/tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc b/tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc
index 18e3fdf3da1..99602ff571e 100644
--- a/tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc
+++ b/tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc
@@ -749,6 +749,12 @@ TfLiteStatus BenchmarkTfLiteModel::Init() {
             << " executed by the delegate.";
       }
     }
+
+    /* for GPU_New Cache Saving */
+    if (0==delegate_provider->GetName().compare("GPU_New")) {
+        delegate_provider->DoAfterInitTime(delegate.get());
+    }
+
     owned_delegates_.emplace_back(std::move(delegate));
   }
 
diff --git a/tensorflow/lite/tools/delegates/BUILD b/tensorflow/lite/tools/delegates/BUILD
index b0ddff8c7bb..187602665cf 100644
--- a/tensorflow/lite/tools/delegates/BUILD
+++ b/tensorflow/lite/tools/delegates/BUILD
@@ -44,6 +44,7 @@ cc_library(
         ":delegate_provider_lib",
         ":external_delegate_provider",
         ":gpu_delegate_provider",
+        ":gpu_new_delegate_provider",
         ":hexagon_delegate_provider",
         ":nnapi_delegate_provider",
         ":xnnpack_delegate_provider",
@@ -95,6 +96,17 @@ cc_library(
     alwayslink = 1,
 )
 
+cc_library(
+    name = "gpu_new_delegate_provider",
+    srcs = ["gpu_new_delegate_provider.cc"],
+    deps = [
+        ":delegate_provider_hdr",
+        "//tensorflow/lite/tools/evaluation:utils",
+        "//tensorflow/lite/delegates/gpu/cl:gpu_api_delegate",
+    ],
+    alwayslink = 1,
+)
+
 cc_library(
     name = "nnapi_delegate_provider",
     srcs = ["nnapi_delegate_provider.cc"],
diff --git a/tensorflow/lite/tools/delegates/delegate_provider.h b/tensorflow/lite/tools/delegates/delegate_provider.h
index 101ce5e88cb..466354a491d 100644
--- a/tensorflow/lite/tools/delegates/delegate_provider.h
+++ b/tensorflow/lite/tools/delegates/delegate_provider.h
@@ -60,6 +60,8 @@ class DelegateProvider {
 
   const ToolParams& DefaultParams() const { return default_params_; }
 
+  virtual void DoAfterInitTime(TfLiteDelegate *delegate) const {}
+
  protected:
   template <typename T>
   Flag CreateFlag(const char* name, ToolParams* params,
diff --git a/tensorflow/lite/tools/delegates/gpu_new_delegate_provider.cc b/tensorflow/lite/tools/delegates/gpu_new_delegate_provider.cc
new file mode 100644
index 00000000000..115fa6c7a2e
--- /dev/null
+++ b/tensorflow/lite/tools/delegates/gpu_new_delegate_provider.cc
@@ -0,0 +1,186 @@
+/* Copyright 2022 The LG Electronics. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "tensorflow/lite/tools/evaluation/utils.h"
+#include "tensorflow/lite/tools/delegates/delegate_provider.h"
+#include "tensorflow/lite/delegates/gpu/cl/gpu_api_delegate.h"
+#include <fstream>
+#include <experimental/filesystem>
+#include <string>
+
+namespace tflite {
+namespace tools {
+
+class GpuNewDelegateProvider : public DelegateProvider {
+ public:
+  GpuNewDelegateProvider() {
+    default_params_.AddParam("use_gpu_new", ToolParam::Create<bool>(false));
+    default_params_.AddParam("precision_loss_allowed", ToolParam::Create<bool>(true));
+    default_params_.AddParam("inference_priority",
+                             ToolParam::Create<std::string>("MAX_PRECISION"));
+    default_params_.AddParam("binary_cache_path", ToolParam::Create<std::string>(""));
+    TFLITE_LOG(INFO) << "Create GpuNewDelegateProvider";
+  }
+
+  std::vector<Flag> CreateFlags(ToolParams* params) const final;
+
+  void LogParams(const ToolParams& params, bool verbose) const final;
+
+  TfLiteDelegatePtr CreateTfLiteDelegate(const ToolParams& params) const final;
+  std::pair<TfLiteDelegatePtr, int> CreateRankedTfLiteDelegate(
+      const ToolParams& params) const final;
+
+  std::string GetName() const final { return "GPU_New"; }
+
+  void DoAfterInitTime(TfLiteDelegate* delegate) const final;
+
+ private:
+  int32_t  getInferencePriority(std::string priority) const;
+  uint8_t* getBinaryCacheData(const std::string &filepath, size_t &filesize) const;
+  void saveBinaryCacheData(TfLiteDelegate* delegate) const;
+  static std::string filePath4Saving;
+};
+REGISTER_DELEGATE_PROVIDER(GpuNewDelegateProvider);
+
+std::string GpuNewDelegateProvider::filePath4Saving = "";
+
+std::vector<Flag> GpuNewDelegateProvider::CreateFlags(ToolParams* params) const {
+  std::vector<Flag> flags = {
+    CreateFlag<bool>("use_gpu_new", params, "use gpu_new"),
+    CreateFlag<bool>("precision_loss_allowed", params,
+                      "Allow to process computation in lower precision than "
+                      "FP32 in GPU. default is true."),
+    CreateFlag<std::string>("inference_priority", params,
+                            "inference priority. default is MAX_PRECISION."),
+    CreateFlag<std::string>("binary_cache_path", params,
+                             "Compiled binary path which contains data returned from"
+                             "TfLiteGpuDelegateGetSerializedBinaryCache call"),
+  };
+  return flags;
+}
+
+void GpuNewDelegateProvider::LogParams(const ToolParams& params,
+                                       bool verbose) const {
+  LOG_TOOL_PARAM(params, bool, "use_gpu_new", "Use gpu_new", verbose);
+  LOG_TOOL_PARAM(params, bool, "precision_loss_allowed", "Allow lower precision in gpu_new", verbose);
+  LOG_TOOL_PARAM(params, std::string, "inference_priority", "Inference Priority", verbose);
+  LOG_TOOL_PARAM(params, std::string, "binary_cache_path", "Compiled binary path", verbose);
+}
+
+TfLiteDelegatePtr GpuNewDelegateProvider::CreateTfLiteDelegate(
+                                          const ToolParams& params) const {
+  TfLiteDelegatePtr delegate(nullptr, [](TfLiteDelegate*) {});
+
+  if (params.Get<bool>("use_gpu_new")) {
+    TfLiteGpuDelegateOptions_New gpu_opts = {0,};
+    gpu_opts.compile_options.inference_priority = TfLiteGpuInferencePriority::
+                                                  TFLITE_GPU_INFERENCE_PRIORITY_MAX_PRECISION;
+    if (params.Get<bool>("precision_loss_allowed")) {
+      gpu_opts.compile_options.precision_loss_allowed = 1;
+    }
+
+    std::string priority = params.Get<std::string>("inference_priority");
+    if (!priority.empty()) {
+        gpu_opts.compile_options.inference_priority = getInferencePriority(priority);
+    }
+
+    std::string filepath = params.Get<std::string>("binary_cache_path");
+    if (!filepath.empty()) {
+      if (std::experimental::filesystem::exists(filepath)) { /* load cache data */
+        gpu_opts.serialized_binary_cache_data = getBinaryCacheData(filepath,
+                                                                   gpu_opts.serialized_binary_cache_size);
+        TFLITE_LOG(INFO) << "Loaded compiled binary cache data!!";
+      } else { /* save cache data */
+        filePath4Saving = filepath;
+        TFLITE_LOG(INFO) << "Will Save the compiled binary cache data after init!!";
+      }
+    }
+    delegate = evaluation::CreateGPUNewDelegate(&gpu_opts);
+    if (!delegate.get()) {
+      TFLITE_LOG(WARN) << "The GPU New acceleration is unsupported on this platform.";
+    }
+  }
+  return delegate;
+}
+
+void GpuNewDelegateProvider::DoAfterInitTime(TfLiteDelegate* delegate) const
+{
+  if (!filePath4Saving.empty())
+    saveBinaryCacheData(delegate);
+}
+
+void GpuNewDelegateProvider::saveBinaryCacheData(TfLiteDelegate* delegate) const
+{
+  size_t size;
+  const uint8_t* data;
+  TfLiteGpuDelegateGetSerializedBinaryCache(delegate, &size, &data);
+
+  std::ofstream outputStream(filePath4Saving, std::ofstream::binary);
+  outputStream.write(reinterpret_cast<const char*>(data), size);
+
+  outputStream.close();
+  filePath4Saving.clear();
+}
+
+uint8_t* GpuNewDelegateProvider::getBinaryCacheData(const std::string &filepath, size_t &filesize) const
+{
+  if (std::experimental::filesystem::is_regular_file(filepath)) {
+    std::ifstream inputStream(filepath, std::ifstream::binary);
+    inputStream.seekg(0, inputStream.end);
+    filesize = inputStream.tellg();
+    inputStream.seekg(0, inputStream.beg);
+
+    char* buffer = new char [filesize];
+    inputStream.read(buffer, filesize);
+    if (inputStream) {
+        TFLITE_LOG(INFO) << "read successfully " << filesize << "bytes.";
+        inputStream.close();
+        return reinterpret_cast<uint8_t*>(buffer); // who deletes it later??? TODO:
+    }
+    else {
+        TFLITE_LOG(ERROR) << "error: read only " << inputStream.gcount() << "bytes.";
+        inputStream.close();
+        return nullptr;
+    }
+  }
+  return nullptr;
+}
+
+int32_t GpuNewDelegateProvider::getInferencePriority(std::string priority) const
+{
+  TFLITE_LOG(INFO) << __func__ << ": " << priority.c_str() << std::endl;
+
+  if (!priority.compare("MAX_PRECISION")) {
+    return TfLiteGpuInferencePriority::TFLITE_GPU_INFERENCE_PRIORITY_MAX_PRECISION;
+  }
+  if (!priority.compare("MIN_LATENCY")) {
+    return TfLiteGpuInferencePriority::TFLITE_GPU_INFERENCE_PRIORITY_MIN_LATENCY;
+  }
+  if (!priority.compare("MIN_MEMORY")) {
+    return TfLiteGpuInferencePriority::TFLITE_GPU_INFERENCE_PRIORITY_MIN_MEMORY_USAGE;
+  }
+
+  return TfLiteGpuInferencePriority::TFLITE_GPU_INFERENCE_PRIORITY_MAX_PRECISION;
+}
+
+std::pair<TfLiteDelegatePtr, int>
+GpuNewDelegateProvider::CreateRankedTfLiteDelegate(
+    const ToolParams& params) const {
+  auto ptr = CreateTfLiteDelegate(params);
+  return std::make_pair(std::move(ptr), params.GetPosition<bool>("use_gpu_new"));
+}
+
+}  // namespace tools
+}  // namespace tflite
diff --git a/tensorflow/lite/tools/evaluation/BUILD b/tensorflow/lite/tools/evaluation/BUILD
index ba745c6a947..3fe932bfd28 100644
--- a/tensorflow/lite/tools/evaluation/BUILD
+++ b/tensorflow/lite/tools/evaluation/BUILD
@@ -43,6 +43,7 @@ cc_library(
         "//tensorflow/lite/c:common",
         "//tensorflow/lite/delegates/nnapi:nnapi_delegate",
         "//tensorflow/lite/delegates/xnnpack:xnnpack_delegate",
+        "//tensorflow/lite/delegates/gpu/cl:gpu_api_delegate",
     ] + select({
         "//tensorflow/lite/delegates/gpu:supports_gpu_delegate": [
             "//tensorflow/lite/delegates/gpu:delegate",
diff --git a/tensorflow/lite/tools/evaluation/utils.cc b/tensorflow/lite/tools/evaluation/utils.cc
index ec09394eb02..a0cdc0d72c1 100644
--- a/tensorflow/lite/tools/evaluation/utils.cc
+++ b/tensorflow/lite/tools/evaluation/utils.cc
@@ -135,6 +135,14 @@ TfLiteDelegatePtr CreateGPUDelegate() {
 #endif  // TFLITE_SUPPORTS_GPU_DELEGATE
 }
 
+#if TFLITE_SUPPORTS_GPU_DELEGATE
+TfLiteDelegatePtr CreateGPUNewDelegate(TfLiteGpuDelegateOptions_New* options)
+{
+  return TfLiteDelegatePtr(TfLiteGpuDelegateCreate_New(options),
+                           &TfLiteGpuDelegateDelete_New);
+}
+#endif // TFLITE_SUPPORTS_GPU_DELEGATE
+
 TfLiteDelegatePtr CreateHexagonDelegate(
     const std::string& library_directory_path, bool profiling) {
 #if defined(__ANDROID__) && (defined(__arm__) || defined(__aarch64__))
diff --git a/tensorflow/lite/tools/evaluation/utils.h b/tensorflow/lite/tools/evaluation/utils.h
index 4a25502b5e6..a997afbdce5 100644
--- a/tensorflow/lite/tools/evaluation/utils.h
+++ b/tensorflow/lite/tools/evaluation/utils.h
@@ -29,6 +29,7 @@ limitations under the License.
 
 #if TFLITE_SUPPORTS_GPU_DELEGATE
 #include "tensorflow/lite/delegates/gpu/delegate.h"
+#include "tensorflow/lite/delegates/gpu/cl/gpu_api_delegate.h"
 #endif
 
 #if defined(__ANDROID__) && (defined(__arm__) || defined(__aarch64__))
@@ -73,6 +74,7 @@ TfLiteDelegatePtr CreateNNAPIDelegate(StatefulNnApiDelegate::Options options);
 TfLiteDelegatePtr CreateGPUDelegate();
 #if TFLITE_SUPPORTS_GPU_DELEGATE
 TfLiteDelegatePtr CreateGPUDelegate(TfLiteGpuDelegateOptionsV2* options);
+TfLiteDelegatePtr CreateGPUNewDelegate(TfLiteGpuDelegateOptions_New* options);
 #endif
 
 TfLiteDelegatePtr CreateHexagonDelegate(
-- 
2.17.1

